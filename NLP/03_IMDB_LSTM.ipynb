{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd0a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9e17a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\arthur\\\\Documents\\\\Data Science Club\\\\cxc-2022\\\\NLP\\\\IMDB Dataset.csv\"\n",
    "\n",
    "raw_ds = pd.read_csv(data_path)\n",
    "raw_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac7f39bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds[\"sentiment\"].value_counts()\n",
    "\n",
    "# perfectly balanced as all things should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e488e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  sentiment_num\n",
       "0  One of the other reviewers has mentioned that ...  positive              1\n",
       "1  A wonderful little production. <br /><br />The...  positive              1\n",
       "2  I thought this was a wonderful way to spend ti...  positive              1\n",
       "3  Basically there's a family where a little boy ...  negative              0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive              1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dict = {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0\n",
    "}\n",
    "\n",
    "raw_ds[\"sentiment_num\"] = raw_ds[\"sentiment\"].map(sentiment_dict)\n",
    "raw_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2a7e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production br br the filming technique is very unassuming very oldtimebbc fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece br br the actors are extremely well chosen michael sheen not only has got all the polari but he has all the voices down pat too you can truly see the seamless editing guided by the references to williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece a masterful production about one of the great masters of comedy and his life br br the realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears it plays on our knowledge and our senses particularly with the scenes concerning orton and halliwell and the sets particularly of their flat with halliwells murals decorating every surface are terribly well done'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cleanup(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "cleanup(raw_ds[\"review\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29449326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove_path = \"C:\\\\Users\\\\arthur\\\\Documents\\\\Data Science Club\\\\cxc-2022\\\\NLP\\\\glove.6B.50d.txt\"\n",
    "glove_dict = dict()\n",
    "glove_word2idx = dict()\n",
    "glove_vect = []\n",
    "GLOVE_DIM = 50\n",
    "with open(glove_path,'r', encoding=\"utf8\") as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], np.float32)\n",
    "        glove_dict[word] = vector\n",
    "        glove_word2idx[word] = idx\n",
    "        glove_vect.append(vector)\n",
    "        \n",
    "glove_vect = np.stack(glove_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df191b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_embedding = np.zeros((1,GLOVE_DIM))   #embedding for '<pad>' token.\n",
    "unk_embedding = np.mean(glove_vect, axis=0, keepdims=True)    #embedding for '<unk>' token.\n",
    "glove_vect = np.vstack((glove_vect, pad_embedding, unk_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b19bbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400002, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bb80b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: 13075 \n",
      "\n",
      "kjnkejnv: 400001\n"
     ]
    }
   ],
   "source": [
    "def vectorize_glove(word):\n",
    "    if word in glove_dict.keys():\n",
    "        return glove_word2idx[word]\n",
    "    else:\n",
    "        return len(glove_vect) - 1 #400000\n",
    "    \n",
    "print(\"hello: {} \\n\\nkjnkejnv: {}\".format(vectorize_glove(\"hello\"), vectorize_glove(\"kjnkejnv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e08e7f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 37668,    835,     41, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000, 400000, 400000, 400000, 400000, 400000, 400000,\n",
       "       400000, 400000])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENT_LEN = 50\n",
    "\n",
    "def vectorize_sentence(sentence):\n",
    "    sent_vec = []\n",
    "    words = list(reversed(nltk.word_tokenize(sentence)))\n",
    "    for i in range(MAX_SENT_LEN):\n",
    "        if i < len(words):\n",
    "            sent_vec.append(vectorize_glove(words[i]))\n",
    "        else:\n",
    "            sent_vec.append(len(glove_vect) - 2)\n",
    "            \n",
    "    return np.stack(sent_vec)\n",
    "\n",
    "vectorize_sentence(cleanup(\"I love maths\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52387619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, isTest=False):\n",
    "        self.raw_ds = pd.read_csv(data_path)\n",
    "        sentiment_dict = {\n",
    "            \"positive\": 1,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "\n",
    "        self.raw_ds[\"sentiment_num\"] = self.raw_ds[\"sentiment\"].map(sentiment_dict)\n",
    "        \n",
    "        train_ds, test_ds = train_test_split(self.raw_ds, test_size=0.2, random_state=42)\n",
    "        if isTest:\n",
    "            self.ds = test_ds\n",
    "        else:\n",
    "            self.ds = train_ds\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.ds[\"review\"].iloc[idx]\n",
    "        y = self.ds[\"sentiment_num\"].iloc[idx]\n",
    "        return vectorize_sentence(cleanup(x)), y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "train_sent = SentimentDataset()\n",
    "test_sent = SentimentDataset(False)\n",
    "\n",
    "train_iter = DataLoader(train_sent, batch_size=40, shuffle=True)\n",
    "test_iter = DataLoader(test_sent, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f54f3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(torch.nn.Module):  \n",
    "    def __init__(self, \n",
    "                 in_dim, \n",
    "                 feature_dim = 50,\n",
    "                 hidden_dim = 256,\n",
    "                 out_dim = 2):\n",
    "        super(BiLSTM,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = torch.nn.Embedding(400000 + 3, 50).from_pretrained(torch.tensor(glove_vect, dtype=torch.float64))\n",
    "        self.lstm = torch.nn.LSTM(feature_dim, hidden_dim, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3) \n",
    "        self.dense = torch.nn.Linear(hidden_dim * 2, out_dim)\n",
    "        \n",
    "    def forward(self, x):   \n",
    "        h0, c0 = self.init_hidden(x)\n",
    "        x = self.dropout(self.embedding(x))\n",
    "        x = torch.squeeze(torch.unsqueeze(x, 0))\n",
    "        out, (hidden,_) = self.lstm(x, (h0,c0))\n",
    "      \n",
    "        x = self.dropout(out[:, -1, :])\n",
    "        return self.dense(x)\n",
    "    \n",
    "    def init_hidden(self, x):\n",
    "        h0 = torch.zeros(4, x.size(0), self.hidden_dim, dtype=torch.double).to(device)\n",
    "        c0 = torch.zeros(4, x.size(0), self.hidden_dim, dtype=torch.double).to(device)\n",
    "        return h0, c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36467bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5140bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = BiLSTM(MAX_SENT_LEN)\n",
    "model = model.to(device).double()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=LR)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e703e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "for ep_idx in range(EPOCHS):\n",
    "    model.train()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    for x, y in train_iter:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        og_out = model(x.type(torch.long))\n",
    "        outputs = torch.squeeze(og_out[:, 0])\n",
    "        loss = loss_fn(outputs, y.float())\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        preds = torch.nn.functional.softmax(og_out, dim=1)[:, 0].cpu().detach().numpy() > 0.5\n",
    "        y_np = y.cpu().detach().numpy()\n",
    "\n",
    "        corrects += np.sum(preds==y_np)\n",
    "        total += len(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc.append(corrects/total)\n",
    "    train_loss.append(np.mean(losses))\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_iter:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            og_out = model(x.type(torch.long))\n",
    "            outputs = torch.squeeze(og_out[:, 0])\n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            preds = torch.nn.functional.softmax(og_out, dim=1)[:, 0].cpu().detach().numpy() > 0.5\n",
    "            y_np = y.cpu().detach().numpy()\n",
    "\n",
    "            corrects += np.sum(preds==y_np)\n",
    "            total += len(x)\n",
    "            \n",
    "    test_acc.append(corrects/total)\n",
    "    test_loss.append(np.mean(losses))\n",
    "    \n",
    "    print(\"---EPOCH {}---\".format(ep_idx))\n",
    "    print(\"Train Acc {} || Loss {}\".format(train_acc[-1], train_loss[-1]))\n",
    "    print(\"Test Acc {} || Loss {}\".format(test_acc[-1], test_loss[-1]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "783f0dc3bf06a379a2f542c4fd928c5983ac73b4340721d085c06efdeba9e65f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_workshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
