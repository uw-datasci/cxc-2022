{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd0a77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9e17a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\arthur\\\\Documents\\\\Data Science Club\\\\cxc-2022\\\\NLP\\\\IMDB Dataset.csv\"\n",
    "\n",
    "raw_ds = pd.read_csv(data_path)\n",
    "raw_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7f39bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds[\"sentiment\"].value_counts()\n",
    "\n",
    "# perfectly balanced as all things should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e488e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  sentiment_num\n",
       "0  One of the other reviewers has mentioned that ...  positive              1\n",
       "1  A wonderful little production. <br /><br />The...  positive              1\n",
       "2  I thought this was a wonderful way to spend ti...  positive              1\n",
       "3  Basically there's a family where a little boy ...  negative              0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive              1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dict = {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0\n",
    "}\n",
    "\n",
    "raw_ds[\"sentiment_num\"] = raw_ds[\"sentiment\"].map(sentiment_dict)\n",
    "raw_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab2a7e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production br br the filming technique is very unassuming very oldtimebbc fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece br br the actors are extremely well chosen michael sheen not only has got all the polari but he has all the voices down pat too you can truly see the seamless editing guided by the references to williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece a masterful production about one of the great masters of comedy and his life br br the realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears it plays on our knowledge and our senses particularly with the scenes concerning orton and halliwell and the sets particularly of their flat with halliwells murals decorating every surface are terribly well done'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cleanup(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "cleanup(raw_ds[\"review\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52387619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, AutoModelForSequenceClassification\n",
    "\n",
    "MAX_SENT_LEN = 50\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, isTest=False):\n",
    "        self.raw_ds = pd.read_csv(data_path)\n",
    "        sentiment_dict = {\n",
    "            \"positive\": 1,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "        self.MAX_LEN = MAX_SENT_LEN\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "        \n",
    "        \n",
    "        self.raw_ds[\"sentiment_num\"] = self.raw_ds[\"sentiment\"].map(sentiment_dict)\n",
    "        \n",
    "        train_ds, test_ds = train_test_split(self.raw_ds, test_size=0.2, random_state=42)\n",
    "        if isTest:\n",
    "            self.ds = test_ds\n",
    "        else:\n",
    "            self.ds = train_ds\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.tokenizer(\n",
    "            self.ds[\"review\"].iloc[idx], \n",
    "            truncation=True, \n",
    "            max_length=self.MAX_LEN, \n",
    "            padding='max_length'\n",
    "        )\n",
    "        y = self.ds[\"sentiment_num\"].iloc[idx]\n",
    "        return np.array(x[\"input_ids\"]), np.array(x[\"attention_mask\"]), y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "train_sent = SentimentDataset()\n",
    "test_sent = SentimentDataset(False)\n",
    "\n",
    "train_iter = DataLoader(train_sent, batch_size=40, shuffle=True)\n",
    "test_iter = DataLoader(test_sent, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54f3163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=2, bias=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)\n",
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36467bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5140bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "torch.cuda.manual_seed_all(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, eps=1e-8)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=len(train_iter)*EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e703e92",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arthur\\Documents\\Data Science Club\\cxc-2022\\NLP\\04_IMDB_BERT.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/04_IMDB_BERT.ipynb#ch0000009?line=24'>25</a>\u001b[0m     total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(input_ids)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/04_IMDB_BERT.ipynb#ch0000009?line=26'>27</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/04_IMDB_BERT.ipynb#ch0000009?line=27'>28</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/04_IMDB_BERT.ipynb#ch0000009?line=28'>29</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/04_IMDB_BERT.ipynb#ch0000009?line=30'>31</a>\u001b[0m train_acc\u001b[39m.\u001b[39mappend(corrects\u001b[39m/\u001b[39mtotal)\n",
      "File \u001b[1;32mc:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\torch\\tensor.py:221\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/tensor.py?line=212'>213</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m Tensor \u001b[39mand\u001b[39;00m has_torch_function(relevant_args):\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/tensor.py?line=213'>214</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/tensor.py?line=214'>215</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/tensor.py?line=215'>216</a>\u001b[0m         relevant_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/tensor.py?line=218'>219</a>\u001b[0m         retain_graph\u001b[39m=\u001b[39mretain_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/tensor.py?line=219'>220</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph)\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/tensor.py?line=220'>221</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph)\n",
      "File \u001b[1;32mc:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\torch\\autograd\\__init__.py:130\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/autograd/__init__.py?line=126'>127</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/autograd/__init__.py?line=127'>128</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/autograd/__init__.py?line=129'>130</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/autograd/__init__.py?line=130'>131</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/autograd/__init__.py?line=131'>132</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "for ep_idx in range(EPOCHS):\n",
    "    model.train()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    for input_ids, attention_mask, y in train_iter:\n",
    "        input_ids, attention_mask, y = input_ids.to(device), attention_mask.to(device), y.to(device)\n",
    "        outputs = model(input_ids.type(torch.long), attention_mask)\n",
    "        logits = outputs[\"logits\"]\n",
    "        \n",
    "        logits = torch.squeeze(logits[:, 0])\n",
    "        loss = loss_fn(logits, y.float())\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        preds = logits.cpu().detach().numpy() > 0.5\n",
    "        y_np = y.cpu().detach().numpy()\n",
    "        \n",
    "        corrects += np.sum(preds==y_np)\n",
    "        total += len(input_ids)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc.append(corrects/total)\n",
    "    train_loss.append(np.mean(losses))\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, y in test_iter:\n",
    "            input_ids, attention_mask, y = input_ids.to(device), attention_mask.to(device), y.to(device)\n",
    "            outputs = model(input_ids.type(torch.long), attention_mask)\n",
    "            logits = outputs[\"logits\"]\n",
    "            logits = torch.squeeze(logits[:, 0])\n",
    "            loss = loss_fn(logits, y.float())\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "            preds = logits.cpu().detach().numpy() > 0.5\n",
    "            y_np = y.cpu().detach().numpy()\n",
    "\n",
    "            corrects += np.sum(preds==y_np)\n",
    "            total += len(input_ids)\n",
    "            \n",
    "    test_acc.append(corrects/total)\n",
    "    test_loss.append(np.mean(losses))\n",
    "    \n",
    "    print(\"---EPOCH {}---\".format(ep_idx))\n",
    "    print(\"Train Acc {} || Loss {}\".format(train_acc[-1], train_loss[-1]))\n",
    "    print(\"Test Acc {} || Loss {}\".format(test_acc[-1], test_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b8ba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './saved_transformer.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "547a660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./saved_transformer.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374ab4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "783f0dc3bf06a379a2f542c4fd928c5983ac73b4340721d085c06efdeba9e65f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_workshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
