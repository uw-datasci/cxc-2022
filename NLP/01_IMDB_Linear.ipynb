{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd0a77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arthur\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad9e17a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\arthur\\\\Documents\\\\Data Science Club\\\\cxc-2022\\\\NLP\\\\IMDB Dataset.csv\"\n",
    "\n",
    "raw_ds = pd.read_csv(data_path)\n",
    "raw_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e61e8a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds.iloc[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8240af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds.iloc[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac7f39bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    25000\n",
       "negative    25000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_ds[\"sentiment\"].value_counts()\n",
    "\n",
    "# perfectly balanced as all things should be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e488e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentiment_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment  sentiment_num\n",
       "0  One of the other reviewers has mentioned that ...  positive              1\n",
       "1  A wonderful little production. <br /><br />The...  positive              1\n",
       "2  I thought this was a wonderful way to spend ti...  positive              1\n",
       "3  Basically there's a family where a little boy ...  negative              0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive              1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dict = {\n",
    "    \"positive\": 1,\n",
    "    \"negative\": 0\n",
    "}\n",
    "\n",
    "raw_ds[\"sentiment_num\"] = raw_ds[\"sentiment\"].map(sentiment_dict)\n",
    "raw_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab2a7e8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a wonderful little production br br the filming technique is very unassuming very oldtimebbc fashion and gives a comforting and sometimes discomforting sense of realism to the entire piece br br the actors are extremely well chosen michael sheen not only has got all the polari but he has all the voices down pat too you can truly see the seamless editing guided by the references to williams diary entries not only is it well worth the watching but it is a terrificly written and performed piece a masterful production about one of the great masters of comedy and his life br br the realism really comes home with the little things the fantasy of the guard which rather than use the traditional dream techniques remains solid then disappears it plays on our knowledge and our senses particularly with the scenes concerning orton and halliwell and the sets particularly of their flat with halliwells murals decorating every surface are terribly well done'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def cleanup(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "    return sentence\n",
    "\n",
    "cleanup(raw_ds[\"review\"].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29449326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "glove_path = \"C:\\\\Users\\\\arthur\\\\Documents\\\\Data Science Club\\\\cxc-2022\\\\NLP\\\\glove.6B.50d.txt\"\n",
    "glove_dict = dict()\n",
    "with open(glove_path,'r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], np.float32)\n",
    "        glove_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a6038dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ',', '.', 'of', 'to']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glove_dict.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8c81eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(glove_dict.values())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c986e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d68e2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_tom = glove_dict['tom']\n",
    "vec_annie = glove_dict['annie']\n",
    "vec_monkey = glove_dict['monkey']\n",
    "vec_elephant = glove_dict['elephant']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945cf96b",
   "metadata": {},
   "source": [
    "$\\text{cosine similarity} = \\frac{A\\cdot B}{||A||\\ ||B||}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bedfb93f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5907844305038452"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(vec_tom,vec_annie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a72330db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8717263042926788"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance.cosine(vec_tom,vec_elephant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b90d32fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, mats = glove_dict.keys(), np.stack(list(glove_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63dcaf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "mats_10 = pca.fit_transform(mats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1da7372",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict_10 = dict()\n",
    "for i, word in enumerate(words):\n",
    "    glove_dict_10[word] = mats_10[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bb80b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello: [ 1.7206483   0.85158     0.16645655 -1.4108487   0.96214426 -0.09164289\n",
      " -1.3437817   0.32029006 -1.0666455  -0.39704138] \n",
      "\n",
      "kjnkejnv: [0.92091484 0.71006758 0.36536972 0.1234569  0.70166822 0.14426783\n",
      " 0.34768384 0.3992395  0.2510057  0.77022236]\n"
     ]
    }
   ],
   "source": [
    "def vectorize_glove(word):\n",
    "    if word in glove_dict.keys():\n",
    "        return glove_dict_10[word]\n",
    "    else:\n",
    "        return np.random.random((10))\n",
    "    \n",
    "print(\"hello: {} \\n\\nkjnkejnv: {}\".format(vectorize_glove(\"hello\"), vectorize_glove(\"kjnkejnv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e08e7f44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAX_SENT_LEN = 50\n",
    "\n",
    "def vectorize_sentence(sentence):\n",
    "    sent_vec = []\n",
    "    words = list(reversed(nltk.word_tokenize(sentence)))\n",
    "    for i in range(MAX_SENT_LEN):\n",
    "        if i < len(words):\n",
    "            sent_vec.append(vectorize_glove(words[i]))\n",
    "        else:\n",
    "            sent_vec.append(np.zeros(10))\n",
    "            \n",
    "    return np.stack(sent_vec)\n",
    "\n",
    "sentence =vectorize_sentence(cleanup(\"I love maths\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2efbb358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 10)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52387619",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, isTest=False):\n",
    "        self.raw_ds = pd.read_csv(data_path)\n",
    "        sentiment_dict = {\n",
    "            \"positive\": 1,\n",
    "            \"negative\": 0\n",
    "        }\n",
    "\n",
    "        self.raw_ds[\"sentiment_num\"] = self.raw_ds[\"sentiment\"].map(sentiment_dict)\n",
    "        \n",
    "        train_ds, test_ds = train_test_split(self.raw_ds, test_size=0.2, random_state=42)\n",
    "        if isTest:\n",
    "            self.ds = test_ds\n",
    "        else:\n",
    "            self.ds = train_ds\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.ds[\"review\"].iloc[idx]\n",
    "        y = self.ds[\"sentiment_num\"].iloc[idx]\n",
    "        return vectorize_sentence(cleanup(x)).flatten(), y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "    \n",
    "train_sent = SentimentDataset()\n",
    "test_sent = SentimentDataset(False)\n",
    "\n",
    "train_iter = DataLoader(train_sent, batch_size=40, shuffle=True)\n",
    "test_iter = DataLoader(test_sent, batch_size=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f54f3163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearNet(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_shape,128)\n",
    "        self.fc2 = nn.Linear(128,32)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(32,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.drop(self.fc3(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36467bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "LR = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5140bd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.manual_seed_all(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = LinearNet(50 * 10)\n",
    "model = model.to(device)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=LR)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e703e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---EPOCH 0---\n",
      "Train Acc 0.504975 || Loss 0.6931109428405762\n",
      "Test Acc 0.5146 || Loss 0.6919792294502258\n",
      "---EPOCH 1---\n",
      "Train Acc 0.525125 || Loss 0.6908968687057495\n",
      "Test Acc 0.542675 || Loss 0.6889687180519104\n",
      "---EPOCH 2---\n",
      "Train Acc 0.537375 || Loss 0.6884753704071045\n",
      "Test Acc 0.55835 || Loss 0.6850482225418091\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\arthur\\Documents\\Data Science Club\\cxc-2022\\NLP\\01_IMDB_Linear.ipynb Cell 26'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000024?line=38'>39</a>\u001b[0m \u001b[39m# validation\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000024?line=39'>40</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000024?line=40'>41</a>\u001b[0m     \u001b[39mfor\u001b[39;00m x, y \u001b[39min\u001b[39;00m test_iter:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000024?line=41'>42</a>\u001b[0m         x, y \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000024?line=42'>43</a>\u001b[0m         outputs \u001b[39m=\u001b[39m model(x\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[1;32mc:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:435\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=432'>433</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=433'>434</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=434'>435</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=435'>436</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=436'>437</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=437'>438</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=438'>439</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:475\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=472'>473</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=473'>474</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=474'>475</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=475'>476</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/dataloader.py?line=476'>477</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=43'>44</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=44'>45</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=45'>46</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=41'>42</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=43'>44</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=44'>45</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/torch/utils/data/_utils/fetch.py?line=45'>46</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32mc:\\Users\\arthur\\Documents\\Data Science Club\\cxc-2022\\NLP\\01_IMDB_Linear.ipynb Cell 22'\u001b[0m in \u001b[0;36mSentimentDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000020?line=20'>21</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds[\u001b[39m\"\u001b[39m\u001b[39mreview\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[idx]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000020?line=21'>22</a>\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mds[\u001b[39m\"\u001b[39m\u001b[39msentiment_num\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39miloc[idx]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000020?line=22'>23</a>\u001b[0m \u001b[39mreturn\u001b[39;00m vectorize_sentence(cleanup(x))\u001b[39m.\u001b[39mflatten(), y\n",
      "\u001b[1;32mc:\\Users\\arthur\\Documents\\Data Science Club\\cxc-2022\\NLP\\01_IMDB_Linear.ipynb Cell 20'\u001b[0m in \u001b[0;36mvectorize_sentence\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000018?line=8'>9</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000018?line=9'>10</a>\u001b[0m         sent_vec\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mzeros(\u001b[39m10\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/arthur/Documents/Data%20Science%20Club/cxc-2022/NLP/01_IMDB_Linear.ipynb#ch0000018?line=11'>12</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mstack(sent_vec)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\arthur\\miniconda3\\envs\\nlp_workshop\\lib\\site-packages\\numpy\\core\\shape_base.py:433\u001b[0m, in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/numpy/core/shape_base.py?line=430'>431</a>\u001b[0m sl \u001b[39m=\u001b[39m (\u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m),) \u001b[39m*\u001b[39m axis \u001b[39m+\u001b[39m (_nx\u001b[39m.\u001b[39mnewaxis,)\n\u001b[0;32m    <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/numpy/core/shape_base.py?line=431'>432</a>\u001b[0m expanded_arrays \u001b[39m=\u001b[39m [arr[sl] \u001b[39mfor\u001b[39;00m arr \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m--> <a href='file:///c%3A/Users/arthur/miniconda3/envs/nlp_workshop/lib/site-packages/numpy/core/shape_base.py?line=432'>433</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(expanded_arrays, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout)\n",
      "File \u001b[1;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_acc = []\n",
    "train_loss = []\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "\n",
    "for ep_idx in range(EPOCHS):\n",
    "    model.train()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    for x, y in train_iter:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        outputs = model(x.float())\n",
    "        outputs = torch.squeeze(outputs)\n",
    "        loss = loss_fn(outputs, y.float())\n",
    "        losses.append(loss.detach().cpu().numpy())\n",
    "        preds = outputs.detach().cpu().numpy() > 0.5\n",
    "        y_np = y.clone().cpu().numpy()\n",
    "        \n",
    "        corrects += np.sum(preds==y_np)\n",
    "        total += len(x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_acc.append(corrects/total)\n",
    "    train_loss.append(np.mean(losses))\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    model.eval()\n",
    "    corrects = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_iter:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            outputs = model(x.float())\n",
    "            outputs = torch.squeeze(outputs)\n",
    "            loss = loss_fn(outputs, y.float())\n",
    "            losses.append(loss.detach().cpu().numpy())\n",
    "            preds = outputs.detach().cpu().numpy() > 0.5\n",
    "            y_np = y.cpu().numpy()\n",
    "\n",
    "            corrects += np.sum(preds==y_np)\n",
    "            total += len(x)\n",
    "            \n",
    "    test_acc.append(corrects/total)\n",
    "    test_loss.append(np.mean(losses))\n",
    "    \n",
    "    print(\"---EPOCH {}---\".format(ep_idx))\n",
    "    print(\"Train Acc {} || Loss {}\".format(train_acc[-1], train_loss[-1]))\n",
    "    print(\"Test Acc {} || Loss {}\".format(test_acc[-1], test_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40f7b22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "783f0dc3bf06a379a2f542c4fd928c5983ac73b4340721d085c06efdeba9e65f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp_workshop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
